{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无标记数据的预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.4.3\n",
      "numpy version: 1.22.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.4.1\n",
      "tensorflow version: 2.13.1\n",
      "2.4.1+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "\t\t\"numpy\", \n",
    "\t\t\"tiktoken\", \n",
    "\t\t\"torch\",\n",
    "\t\t\"tensorflow\" # For OpenAI's pretrained weights\n",
    "\t\t]\n",
    "for p in pkgs:\n",
    "\tprint(f\"{p} version: {version(p)}\")\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Evaluating generative text models\n",
    "\n",
    "#### 5.1.1 GPT生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未有k v cache\n",
    "\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "\t\"vocab_size\": 50257,   # Vocabulary size\n",
    "\t\"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "\t\"emb_dim\": 768,        # Embedding dimension\n",
    "\t\"n_heads\": 12,         # Number of attention heads\n",
    "\t\"n_layers\": 12,        # Number of layers\n",
    "\t\"drop_rate\": 0.1,      # Dropout rate\n",
    "\t\"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference\t# 全局影响 可影响其他cell model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "\tencoded = tokenizer.encode(text, allowed_special={\"<|endoftext|\"})\n",
    "\tencoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\treturn encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "\tflat = token_ids.squeeze(0)\n",
    "\treturn tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "\tmodel=model,\n",
    "\tidx=text_to_token_ids(start_context, tokenizer),\n",
    "\tmax_new_tokens=10,\n",
    "\tcontext_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 loss 交叉熵 perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "\t\t\t\t\t\t[40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "\t\t\t\t\t\t[1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# 通过model + softmax -> 概率\n",
    "with torch.no_grad():\n",
    "\tlogits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n",
    "# 每个batch 中的 context_length 对应的50257为词表大小 其数值为对应该词的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将概率转回文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs batch 1: every effort moves\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inputs batch 1: {token_ids_to_text(inputs[0], tokenizer)}\")\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n",
      "torch.Size([3, 50257])\n",
      "torch.Size([3, 50257])\n",
      "torch.Size([3, 50257])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "# target_probas_1 = probas[text_idx, :, targets[text_idx]]\t# :会进行广播 导致结果为3*3 对每个词表概率都取了targets[text_idx]的三个值\n",
    "\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)\n",
    "\n",
    "a = probas[text_idx]\n",
    "print(a.shape)\n",
    "b = a[[0,1,2]]\n",
    "print(b.shape)\n",
    "c = a[:]\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标是令以上三值达到1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# 取对数\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "\n",
    "# 求平均\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标变为 令平均对数的值尽可能的大 上限为0\n",
    "\n",
    "- *-1 变为尽可能小 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交叉熵 流程\n",
    "\n",
    "model+softmax求概率 $\\rightarrow$ 取出目标的概率 $\\rightarrow$ 取对数 $\\rightarrow$ 求平均 $\\rightarrow$ *-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 困惑度 exponential\n",
    "\n",
    "- perplexity = exp(交叉熵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 training validation set loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"../ch02/the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "\twith urllib.request.urlopen(url) as response:\n",
    "\t\ttext_data = response.read().decode('utf-8')\n",
    "\twith open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "\t\tfile.write(text_data)\n",
    "else:\n",
    "\twith open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "\t\ttext_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# First 99 characters\n",
    "print(text_data[:99])\n",
    "# Last 99 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)           # 设置CPU随机种子\n",
    "torch.cuda.manual_seed(123)      # 设置当前GPU随机种子\n",
    "torch.cuda.manual_seed_all(123)  # 设置所有GPU随机种子（多卡时）\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "\ttrain_data,\n",
    "\tbatch_size=2,\n",
    "\tmax_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "\tstride=GPT_CONFIG_124M[\"context_length\"],\n",
    "\tdrop_last=True,\n",
    "\tshuffle=True,\n",
    "\tnum_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "\t\"\"\"\n",
    "\t计算给定批次的交叉熵\n",
    "\t\"\"\"\n",
    "\tinput_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\tlogits = model(input_batch)\n",
    "\tloss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "\treturn loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "\ttotal_loss=0.\n",
    "\tif len(data_loader) == 0:\n",
    "\t\treturn float(\"nan\")\n",
    "\t\n",
    "\tif num_batches is None:\n",
    "\t\tnum_batches = len(data_loader)\n",
    "\telse:\t# 防止batch的量比所有数量都多\n",
    "\t\tnum_batches = min(num_batches, len(data_loader))\n",
    "\t\n",
    "\tfor i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "\t\tif i >= num_batches:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tloss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\t\ttotal_loss += loss.item()\n",
    "\n",
    "\treturn total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758273654514\n",
      "Validation loss: 10.981104850769043\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\ttrain_loss = calc_loss_loader(train_loader, model, device)\n",
    "\tval_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, \n",
    "\t\t\t\t\t\teval_freq, eval_iter, start_context, tokenizer):\n",
    "\ttrain_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\ttokens_seen, global_step = 0, -1\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\tfor input_batch, target_batch in train_loader:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttokens_seen += input_batch.numel()\t# 已训练的token计数\n",
    "\t\t\tglobal_step += 1\n",
    "\n",
    "\t\t\t# Optional evaluation step\n",
    "\t\t\tif global_step % eval_freq == 0:\n",
    "\t\t\t\ttrain_loss, val_loss = evaluate_model(\n",
    "\t\t\t\t\tmodel, train_loader, val_loader, device, eval_iter\n",
    "\t\t\t\t)\n",
    "\t\t\t\ttrain_losses.append(train_loss)\n",
    "\t\t\t\tval_losses.append(val_loss)\n",
    "\t\t\t\ttrack_tokens_seen.append(tokens_seen)\n",
    "\t\t\t\tprint(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "\t\t\t\t\t\tf\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\t\n",
    "\t\t# 每个epoch之后打印示例文本\n",
    "\t\tgenerate_and_print_sample(\n",
    "\t\t\tmodel, tokenizer, device, start_context\n",
    "\t\t)\n",
    "\n",
    "\treturn train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\ttrain_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "\t\tval_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "\tmodel.train()\n",
    "\treturn train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "\tmodel.eval()\n",
    "\tcontext_size = model.pos_emb.weight.shape[0]\n",
    "\tencoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\ttoken_ids = generate_text_simple(\n",
    "\t\t\tmodel=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "\t\t)\n",
    "\tdecoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\tprint(decoded_text.replace(\"\\n\", \"<|/n|>\"))\n",
    "\tmodel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.818, Val loss 9.928\n",
      "Ep 1 (Step 000005): Train loss 8.065, Val loss 8.335\n",
      "Every effort moves you,,,,,,,,,,,,.<|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|>\n",
      "Ep 2 (Step 000010): Train loss 6.622, Val loss 7.051\n",
      "Ep 2 (Step 000015): Train loss 6.047, Val loss 6.600\n",
      "Every effort moves you, and,, and,,,,,,, and,.<|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|>\n",
      "Ep 3 (Step 000020): Train loss 5.586, Val loss 6.477\n",
      "Ep 3 (Step 000025): Train loss 5.523, Val loss 6.399\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Ep 4 (Step 000030): Train loss 5.128, Val loss 6.366\n",
      "Ep 4 (Step 000035): Train loss 4.941, Val loss 6.366\n",
      "Every effort moves you a a to the a. Gisburn, and a. Gisburn. I had the of the of the of the of the of the of the of the of the of the of the of the of the of the. I had a\n",
      "Ep 5 (Step 000040): Train loss 4.340, Val loss 6.246\n",
      "Every effort moves you, with a, in the of the picture--as of the of the of the of the picture of his of the<|/n|><|/n|>\"I had been.<|/n|>\"Oh, in the donkey--and it was a little the man of the picture of\n",
      "Ep 6 (Step 000045): Train loss 3.967, Val loss 6.181\n",
      "Ep 6 (Step 000050): Train loss 3.451, Val loss 6.155\n",
      "Every effort moves you know the<|/n|>\"Oh, and.<|/n|><|/n|>\"Oh, and in a little: \"There, and in the<|/n|><|/n|><|/n|><|/n|>\"Oh, and I had been the donkey.<|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|>\n",
      "Ep 7 (Step 000055): Train loss 3.466, Val loss 6.195\n",
      "Ep 7 (Step 000060): Train loss 2.666, Val loss 6.134\n",
      "Every effort moves you know the picture.<|/n|><|/n|>\"I looked he was a little the last word.<|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|>\"I he was his pictures-c.<|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|>\n",
      "Ep 8 (Step 000065): Train loss 2.208, Val loss 6.141\n",
      "Ep 8 (Step 000070): Train loss 1.879, Val loss 6.228\n",
      "Every effort moves you know,\" was not that the picture.<|/n|><|/n|>\"I had the last word. Gisburn's an!<|/n|><|/n|>\"Oh, in the moment--as Jack himself, as he was his own the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.499, Val loss 6.230\n",
      "Ep 9 (Step 000080): Train loss 1.174, Val loss 6.250\n",
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fact, with a Mrs. Gisburn's open countenance. \"It's his pictures with a \"strongest,\" she was\n",
      "Ep 10 (Step 000085): Train loss 0.901, Val loss 6.328\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|><|/n|>He placed them at my elbow and as I turned, and down the room, when I\n",
      "Ep 11 (Step 000090): Train loss 0.649, Val loss 6.372\n",
      "Ep 11 (Step 000095): Train loss 0.547, Val loss 6.404\n",
      "Every effort moves you?\"<|/n|>\"I didn't too? I haven't seen a single one in the house.\"<|/n|><|/n|>\"I looked up the cigars you like.\"<|/n|>\"I looked at the donkey again. . . . . . . . . . . .\n",
      "Ep 12 (Step 000100): Train loss 0.393, Val loss 6.468\n",
      "Ep 12 (Step 000105): Train loss 0.274, Val loss 6.491\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 13 (Step 000110): Train loss 0.236, Val loss 6.597\n",
      "Ep 13 (Step 000115): Train loss 0.200, Val loss 6.695\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Ep 14 (Step 000120): Train loss 0.156, Val loss 6.735\n",
      "Ep 14 (Step 000125): Train loss 0.130, Val loss 6.776\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 15 (Step 000130): Train loss 0.094, Val loss 6.810\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 16 (Step 000135): Train loss 0.094, Val loss 6.856\n",
      "Ep 16 (Step 000140): Train loss 0.086, Val loss 6.923\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 17 (Step 000145): Train loss 0.077, Val loss 6.925\n",
      "Ep 17 (Step 000150): Train loss 0.072, Val loss 6.954\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 18 (Step 000155): Train loss 0.065, Val loss 7.007\n",
      "Ep 18 (Step 000160): Train loss 0.052, Val loss 7.017\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 19 (Step 000165): Train loss 0.047, Val loss 7.065\n",
      "Ep 19 (Step 000170): Train loss 0.040, Val loss 7.129\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 20 (Step 000175): Train loss 0.043, Val loss 7.155\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 21 (Step 000180): Train loss 0.040, Val loss 7.174\n",
      "Ep 21 (Step 000185): Train loss 0.029, Val loss 7.205\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 22 (Step 000190): Train loss 0.022, Val loss 7.172\n",
      "Ep 22 (Step 000195): Train loss 0.028, Val loss 7.205\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 23 (Step 000200): Train loss 0.020, Val loss 7.270\n",
      "Ep 23 (Step 000205): Train loss 0.017, Val loss 7.222\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 24 (Step 000210): Train loss 0.019, Val loss 7.253\n",
      "Ep 24 (Step 000215): Train loss 0.015, Val loss 7.295\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 25 (Step 000220): Train loss 0.013, Val loss 7.277\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 26 (Step 000225): Train loss 0.024, Val loss 7.346\n",
      "Ep 26 (Step 000230): Train loss 0.011, Val loss 7.313\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 27 (Step 000235): Train loss 0.017, Val loss 7.398\n",
      "Ep 27 (Step 000240): Train loss 0.019, Val loss 7.288\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 28 (Step 000245): Train loss 0.030, Val loss 7.243\n",
      "Ep 28 (Step 000250): Train loss 0.034, Val loss 7.406\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 29 (Step 000255): Train loss 0.015, Val loss 7.397\n",
      "Ep 29 (Step 000260): Train loss 0.009, Val loss 7.405\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 30 (Step 000265): Train loss 0.009, Val loss 7.389\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 31 (Step 000270): Train loss 0.007, Val loss 7.376\n",
      "Ep 31 (Step 000275): Train loss 0.007, Val loss 7.401\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 32 (Step 000280): Train loss 0.006, Val loss 7.415\n",
      "Ep 32 (Step 000285): Train loss 0.007, Val loss 7.417\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 33 (Step 000290): Train loss 0.005, Val loss 7.408\n",
      "Ep 33 (Step 000295): Train loss 0.006, Val loss 7.418\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 34 (Step 000300): Train loss 0.004, Val loss 7.440\n",
      "Ep 34 (Step 000305): Train loss 0.004, Val loss 7.498\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 35 (Step 000310): Train loss 0.004, Val loss 7.501\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 36 (Step 000315): Train loss 0.004, Val loss 7.470\n",
      "Ep 36 (Step 000320): Train loss 0.003, Val loss 7.473\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 37 (Step 000325): Train loss 0.004, Val loss 7.476\n",
      "Ep 37 (Step 000330): Train loss 0.003, Val loss 7.487\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 38 (Step 000335): Train loss 0.004, Val loss 7.481\n",
      "Ep 38 (Step 000340): Train loss 0.003, Val loss 7.510\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 39 (Step 000345): Train loss 0.003, Val loss 7.522\n",
      "Ep 39 (Step 000350): Train loss 0.004, Val loss 7.510\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 40 (Step 000355): Train loss 0.003, Val loss 7.529\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 41 (Step 000360): Train loss 0.003, Val loss 7.582\n",
      "Ep 41 (Step 000365): Train loss 0.002, Val loss 7.540\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 42 (Step 000370): Train loss 0.002, Val loss 7.528\n",
      "Ep 42 (Step 000375): Train loss 0.002, Val loss 7.539\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 43 (Step 000380): Train loss 0.003, Val loss 7.583\n",
      "Ep 43 (Step 000385): Train loss 0.002, Val loss 7.607\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 44 (Step 000390): Train loss 0.002, Val loss 7.593\n",
      "Ep 44 (Step 000395): Train loss 0.002, Val loss 7.586\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 45 (Step 000400): Train loss 0.002, Val loss 7.595\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 46 (Step 000405): Train loss 0.002, Val loss 7.618\n",
      "Ep 46 (Step 000410): Train loss 0.002, Val loss 7.638\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 47 (Step 000415): Train loss 0.002, Val loss 7.645\n",
      "Ep 47 (Step 000420): Train loss 0.002, Val loss 7.647\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 48 (Step 000425): Train loss 0.002, Val loss 7.645\n",
      "Ep 48 (Step 000430): Train loss 0.002, Val loss 7.649\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 49 (Step 000435): Train loss 0.002, Val loss 7.658\n",
      "Ep 49 (Step 000440): Train loss 0.002, Val loss 7.663\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 50 (Step 000445): Train loss 0.001, Val loss 7.664\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 51 (Step 000450): Train loss 0.001, Val loss 7.666\n",
      "Ep 51 (Step 000455): Train loss 0.001, Val loss 7.671\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 52 (Step 000460): Train loss 0.001, Val loss 7.676\n",
      "Ep 52 (Step 000465): Train loss 0.001, Val loss 7.681\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 53 (Step 000470): Train loss 0.001, Val loss 7.684\n",
      "Ep 53 (Step 000475): Train loss 0.001, Val loss 7.686\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 54 (Step 000480): Train loss 0.001, Val loss 7.684\n",
      "Ep 54 (Step 000485): Train loss 0.001, Val loss 7.685\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 55 (Step 000490): Train loss 0.001, Val loss 7.688\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 56 (Step 000495): Train loss 0.001, Val loss 7.694\n",
      "Ep 56 (Step 000500): Train loss 0.001, Val loss 7.698\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 57 (Step 000505): Train loss 0.001, Val loss 7.703\n",
      "Ep 57 (Step 000510): Train loss 0.001, Val loss 7.708\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 58 (Step 000515): Train loss 0.001, Val loss 7.712\n",
      "Ep 58 (Step 000520): Train loss 0.001, Val loss 7.716\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 59 (Step 000525): Train loss 0.001, Val loss 7.719\n",
      "Ep 59 (Step 000530): Train loss 0.001, Val loss 7.725\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 60 (Step 000535): Train loss 0.001, Val loss 7.732\n",
      "Every effort moves you?\"<|/n|><|/n|>\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"<|/n|><|/n|>He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Ep 61 (Step 000540): Train loss 0.001, Val loss 7.738\n",
      "Ep 61 (Step 000545): Train loss 0.001, Val loss 7.743\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21624/2488844554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m train_losses, val_losses, tokens_seen = train_model_simple(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21624/2899609887.py\u001b[0m in \u001b[0;36mtrain_model_simple\u001b[0;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# 每个epoch之后打印示例文本\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \t\tgenerate_and_print_sample(\n\u001b[0m\u001b[1;32m     30\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \t\t)\n",
      "\u001b[0;32m/tmp/ipykernel_21624/2899609887.py\u001b[0m in \u001b[0;36mgenerate_and_print_sample\u001b[0;34m(model, tokenizer, device, start_context)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_to_token_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \t\ttoken_ids = generate_text_simple(\n\u001b[0m\u001b[1;32m     49\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \t\t)\n",
      "\u001b[0;32m/liuxianguo2/linshuo/LLMs/ch05/previous_chapters.py\u001b[0m in \u001b[0;36mgenerate_text_simple\u001b[0;34m(model, idx, max_new_tokens, context_size)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;31m# Get the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Focus only on the last time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/liuxianguo2/linshuo/LLMs/ch05/previous_chapters.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in_idx)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpos_embeds\u001b[0m  \u001b[0;31m# Shape [batch_size, num_tokens, emb_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrf_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/liuxianguo2/linshuo/LLMs/ch05/previous_chapters.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# Shortcut connection for attention block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# Shape [batch_size, num_tokens, emb_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_shortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/liuxianguo2/linshuo/LLMs/ch05/previous_chapters.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munbiased\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mnorm_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.lines import lineStyles\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epoches_seen, tokens_seen, train_losses, val_losses):\n",
    "\tfig, ax1 = plt.subplots(figsize=(5,3))\n",
    "\tax1.plot(epoches_seen, train_losses, label=\"Training loss\")\n",
    "\tax1.plot(epoches_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "\tax1.set_xlabel(\"Epochs\")\n",
    "\tax1.set_ylabel(\"Loss\")\n",
    "\tax1.legend(loc=\"upper right\")\t# 图例位置\n",
    "\n",
    "\tax2 = ax1.twiny()\n",
    "\tax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "\tax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "\tfig.tight_layout()\t# 调整布局\n",
    "\tplt.savefig(\"loss-plot.pdf\")\n",
    "\tplt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
