{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 无标记数据的预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.4.3\n",
      "numpy version: 1.22.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.4.1\n",
      "tensorflow version: 2.13.1\n",
      "2.4.1+cu121\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "\t\t\"numpy\", \n",
    "\t\t\"tiktoken\", \n",
    "\t\t\"torch\",\n",
    "\t\t\"tensorflow\" # For OpenAI's pretrained weights\n",
    "\t\t]\n",
    "for p in pkgs:\n",
    "\tprint(f\"{p} version: {version(p)}\")\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Evaluating generative text models\n",
    "\n",
    "#### 5.1.1 GPT生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 未有k v cache\n",
    "\n",
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "\t\"vocab_size\": 50257,   # Vocabulary size\n",
    "\t\"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "\t\"emb_dim\": 768,        # Embedding dimension\n",
    "\t\"n_heads\": 12,         # Number of attention heads\n",
    "\t\"n_layers\": 12,        # Number of layers\n",
    "\t\"drop_rate\": 0.1,      # Dropout rate\n",
    "\t\"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference\t# 全局影响 可影响其他cell model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "\tencoded = tokenizer.encode(text, allowed_special={\"<|endoftext|\"})\n",
    "\tencoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "\treturn encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "\tflat = token_ids.squeeze(0)\n",
    "\treturn tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "\tmodel=model,\n",
    "\tidx=text_to_token_ids(start_context, tokenizer),\n",
    "\tmax_new_tokens=10,\n",
    "\tcontext_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 loss 交叉熵 perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "\t\t\t\t\t\t[40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "\t\t\t\t\t\t[1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "# 通过model + softmax -> 概率\n",
    "with torch.no_grad():\n",
    "\tlogits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1)\n",
    "print(probas.shape)\n",
    "# 每个batch 中的 context_length 对应的50257为词表大小 其数值为对应该词的概率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将概率转回文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs batch 1: every effort moves\n",
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Inputs batch 1: {token_ids_to_text(inputs[0], tokenizer)}\")\n",
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n",
      "torch.Size([3, 50257])\n",
      "torch.Size([3, 50257])\n",
      "torch.Size([3, 50257])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "# target_probas_1 = probas[text_idx, :, targets[text_idx]]\t# :会进行广播 导致结果为3*3 对每个词表概率都取了targets[text_idx]的三个值\n",
    "\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)\n",
    "\n",
    "a = probas[text_idx]\n",
    "print(a.shape)\n",
    "b = a[[0,1,2]]\n",
    "print(b.shape)\n",
    "c = a[:]\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标是令以上三值达到1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n",
      "tensor(-10.7940)\n"
     ]
    }
   ],
   "source": [
    "# 取对数\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)\n",
    "\n",
    "# 求平均\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 目标变为 令平均对数的值尽可能的大 上限为0\n",
    "\n",
    "- *-1 变为尽可能小 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 交叉熵 流程\n",
    "\n",
    "model+softmax求概率 $\\rightarrow$ 取出目标的概率 $\\rightarrow$ 取对数 $\\rightarrow$ 求平均 $\\rightarrow$ *-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 困惑度 exponential\n",
    "\n",
    "- perplexity = exp(交叉熵)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48725.8203)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 training validation set loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "file_path = \"../ch02/the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "\twith urllib.request.urlopen(url) as response:\n",
    "\t\ttext_data = response.read().decode('utf-8')\n",
    "\twith open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "\t\tfile.write(text_data)\n",
    "else:\n",
    "\twith open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "\t\ttext_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "# First 99 characters\n",
    "print(text_data[:99])\n",
    "# Last 99 characters\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)           # 设置CPU随机种子\n",
    "torch.cuda.manual_seed(123)      # 设置当前GPU随机种子\n",
    "torch.cuda.manual_seed_all(123)  # 设置所有GPU随机种子（多卡时）\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "\ttrain_data,\n",
    "\tbatch_size=2,\n",
    "\tmax_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "\tstride=GPT_CONFIG_124M[\"context_length\"],\n",
    "\tdrop_last=True,\n",
    "\tshuffle=True,\n",
    "\tnum_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "\t\"\"\"\n",
    "\t计算给定批次的交叉熵\n",
    "\t\"\"\"\n",
    "\tinput_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "\tlogits = model(input_batch)\n",
    "\tloss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "\treturn loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "\ttotal_loss=0.\n",
    "\tif len(data_loader) == 0:\n",
    "\t\treturn float(\"nan\")\n",
    "\t\n",
    "\tif num_batches is None:\n",
    "\t\tnum_batches = len(data_loader)\n",
    "\telse:\t# 防止batch的量比所有数量都多\n",
    "\t\tnum_batches = min(num_batches, len(data_loader))\n",
    "\t\n",
    "\tfor i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "\t\tif i >= num_batches:\n",
    "\t\t\tbreak\n",
    "\n",
    "\t\tloss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\t\ttotal_loss += loss.item()\n",
    "\n",
    "\treturn total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758273654514\n",
      "Validation loss: 10.981104850769043\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "\ttrain_loss = calc_loss_loader(train_loader, model, device)\n",
    "\tval_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training a LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, \n",
    "\t\t\t\t\t\teval_freq, eval_iter, start_context, tokenizer):\n",
    "\ttrain_losses, val_losses, track_tokens_seen = [], [], []\n",
    "\ttokens_seen, global_step = 0, -1\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tmodel.train()\n",
    "\n",
    "\t\tfor input_batch, target_batch in train_loader:\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttokens_seen += input_batch.numel()\t# 已训练的token计数\n",
    "\t\t\tglobal_step += 1\n",
    "\n",
    "\t\t\t# Optional evaluation step\n",
    "\t\t\tif global_step % eval_freq == 0:\n",
    "\t\t\t\ttrain_loss, val_loss = evaluate_model(\n",
    "\t\t\t\t\tmodel, train_loader, val_loader, device, eval_iter\n",
    "\t\t\t\t)\n",
    "\t\t\t\ttrain_losses.append(train_loss)\n",
    "\t\t\t\tval_losses.append(val_loss)\n",
    "\t\t\t\ttrack_tokens_seen.append(tokens_seen)\n",
    "\t\t\t\tprint(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "\t\t\t\t\t\tf\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\t\n",
    "\t\t# 每个epoch之后打印示例文本\n",
    "\t\tgenerate_and_print_sample(\n",
    "\t\t\tmodel, tokenizer, device, start_context\n",
    "\t\t)\n",
    "\n",
    "\treturn train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\ttrain_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "\t\tval_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "\tmodel.train()\n",
    "\treturn train_loss, val_loss\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "\tmodel.eval()\n",
    "\tcontext_size = model.pos_emb.weight.shape[0]\n",
    "\tencoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "\twith torch.no_grad():\n",
    "\t\ttoken_ids = generate_text_simple(\n",
    "\t\t\tmodel=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "\t\t)\n",
    "\tdecoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "\tprint(decoded_text.replace(\"\\n\", \"<|/n|>\"))\n",
    "\tmodel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.818, Val loss 9.928\n",
      "Ep 1 (Step 000005): Train loss 8.065, Val loss 8.335\n",
      "Every effort moves you,,,,,,,,,,,,.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ep 2 (Step 000010): Train loss 6.622, Val loss 7.051\n",
      "Ep 2 (Step 000015): Train loss 6.047, Val loss 6.600\n",
      "Every effort moves you, and,, and,,,,,,, and,.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ep 3 (Step 000020): Train loss 5.586, Val loss 6.477\n",
      "Ep 3 (Step 000025): Train loss 5.523, Val loss 6.399\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Ep 4 (Step 000030): Train loss 5.128, Val loss 6.366\n",
      "Ep 4 (Step 000035): Train loss 4.941, Val loss 6.366\n",
      "Every effort moves you a a to the a. Gisburn, and a. Gisburn. I had the of the of the of the of the of the of the of the of the of the of the of the of the of the. I had a\n",
      "Ep 5 (Step 000040): Train loss 4.340, Val loss 6.246\n",
      "Every effort moves you, with a, in the of the picture--as of the of the of the of the picture of his of the\n",
      "\n",
      "\"I had been.\n",
      "\"Oh, in the donkey--and it was a little the man of the picture of\n",
      "Ep 6 (Step 000045): Train loss 3.967, Val loss 6.181\n",
      "Ep 6 (Step 000050): Train loss 3.451, Val loss 6.155\n",
      "Every effort moves you know the\n",
      "\"Oh, and.\n",
      "\n",
      "\"Oh, and in a little: \"There, and in the\n",
      "\n",
      "\n",
      "\n",
      "\"Oh, and I had been the donkey.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ep 7 (Step 000055): Train loss 3.466, Val loss 6.195\n",
      "Ep 7 (Step 000060): Train loss 2.666, Val loss 6.134\n",
      "Every effort moves you know the picture.\n",
      "\n",
      "\"I looked he was a little the last word.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\"I he was his pictures-c.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ep 8 (Step 000065): Train loss 2.208, Val loss 6.141\n",
      "Ep 8 (Step 000070): Train loss 1.879, Val loss 6.228\n",
      "Every effort moves you know,\" was not that the picture.\n",
      "\n",
      "\"I had the last word. Gisburn's an!\n",
      "\n",
      "\"Oh, in the moment--as Jack himself, as he was his own the donkey. \"There were days when I\n",
      "Ep 9 (Step 000075): Train loss 1.499, Val loss 6.230\n",
      "Ep 9 (Step 000080): Train loss 1.174, Val loss 6.250\n",
      "Every effort moves you know,\" was not that my hostess was \"interesting\": on that point I could have given Miss Croft the fact, with a Mrs. Gisburn's open countenance. \"It's his pictures with a \"strongest,\" she was\n",
      "Ep 10 (Step 000085): Train loss 0.901, Val loss 6.328\n",
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "He placed them at my elbow and as I turned, and down the room, when I\n",
      "Training completed in 0.51 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
