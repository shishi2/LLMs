{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix E: Parameter-efficient Finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.4.3\n",
      "numpy version: 1.23.0\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.4.1\n",
      "tensorflow version: 2.13.1\n",
      "pandas version: 2.0.3\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "\t\t\"numpy\",\n",
    "\t\t\"tiktoken\",\n",
    "\t\t\"torch\",\n",
    "\t\t\"tensorflow\", # For OpenAI's pretrained weights\n",
    "\t\t\"pandas\"      # Dataset loading\n",
    "\t   ]\n",
    "for p in pkgs:\n",
    "\tprint(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from previous_chapters import (\n",
    "\tdownload_and_unzip_spam_data,\n",
    "\tcreate_balanced_dataset,\n",
    "\trandom_split\n",
    ")\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "try:\n",
    "\tdownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "except (urllib.error.HTTPError, urllib.error.URLError, TimeoutError) as e:\n",
    "\tprint(f\"Primary URL failed: {e}. Trying backup URL...\")\n",
    "\turl = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/sms%2Bspam%2Bcollection.zip\"\n",
    "\tdownload_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    " \n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from previous_chapters import SpamDataset\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\tdataset=train_dataset,\n",
    "\tbatch_size=batch_size,\n",
    "\tshuffle=True,\n",
    "\tnum_workers=num_workers,\n",
    "\tdrop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\tdataset=val_dataset,\n",
    "\tbatch_size=batch_size,\n",
    "\tnum_workers=num_workers,\n",
    "\tdrop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "\tdataset=test_dataset,\n",
    "\tbatch_size=batch_size,\n",
    "\tnum_workers=num_workers,\n",
    "\tdrop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n",
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "# 检查数据集\n",
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "\tpass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)\n",
    "\n",
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ../ch05/gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: ../ch05/gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: ../ch05/gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: ../ch05/gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: ../ch05/gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: ../ch05/gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: ../ch05/gpt2/124M/vocab.bpe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../ch05\")\n",
    "\n",
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "# Alternatively:\n",
    "# from llms_from_scratch.ch04 import GPTModel\n",
    "# from llms_from_scratch.ch05 import load_weights_into_gpt\n",
    "\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "\t\"vocab_size\": 50257,     # Vocabulary size\n",
    "\t\"context_length\": 1024,  # Context length\n",
    "\t\"drop_rate\": 0.0,        # Dropout rate\n",
    "\t\"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "\t\"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "\t\"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "\t\"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "\t\"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"../ch05/gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "# 测试模型加载\n",
    "from previous_chapters import (\n",
    "\tgenerate_text_simple,\n",
    "\ttext_to_token_ids,\n",
    "\ttoken_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "\tmodel=model,\n",
    "\tidx=text_to_token_ids(text_1, tokenizer),\n",
    "\tmax_new_tokens=15,\n",
    "\tcontext_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# 未经微调模型测试\n",
    "from previous_chapters import calc_accuracy_loader\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Parameter-efficient finetuning with LoRA\n",
    "\n",
    "- 创建LoRA层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "\tdef __init__(self, in_dim, out_dim, rank, alpha):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "\t\ttorch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\t# 类似于标准的权重初始化\n",
    "\t\tself.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "\t\tself.alpha = alpha\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.alpha * (x @ self.A @ self.B)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LinearWithLoRA 可以替换所有的线性 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "\tdef __init__(self, linear, rank, alpha):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear = linear\n",
    "\t\tself.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\treturn self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "\tfor name, module in model.named_children():\t# 遍历所有直接下属层\n",
    "\t\tif isinstance(module, torch.nn.Linear):\n",
    "\t\t\t# 将Linear换成带LoRA版本的\n",
    "\t\t\tsetattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "\t\telse:\n",
    "\t\t\t# 递归调用\n",
    "\t\t\treplace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 124,441,346\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "# 冻结参数\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "\tparam.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "# 换LoRA\n",
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# 由于初始化化B=0\n",
    "# 故未经训练结果不变\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462\n",
      "Ep 1 (Step 000050): Train loss 0.396, Val loss 0.364\n",
      "Ep 1 (Step 000100): Train loss 0.111, Val loss 0.229\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 2 (Step 000150): Train loss 0.135, Val loss 0.073\n",
      "Ep 2 (Step 000200): Train loss 0.008, Val loss 0.050\n",
      "Ep 2 (Step 000250): Train loss 0.022, Val loss 0.179\n",
      "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.088, Val loss 0.043\n",
      "Ep 3 (Step 000350): Train loss 0.023, Val loss 0.184\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 4 (Step 000400): Train loss 0.038, Val loss 0.017\n",
      "Ep 4 (Step 000450): Train loss 0.014, Val loss 0.070\n",
      "Ep 4 (Step 000500): Train loss 0.000, Val loss 0.131\n",
      "Training accuracy: 100.00% | Validation accuracy: 95.00%\n",
      "Ep 5 (Step 000550): Train loss 0.009, Val loss 0.118\n",
      "Ep 5 (Step 000600): Train loss 0.007, Val loss 0.266\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 1.59 minutes.\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "import time\n",
    "from previous_chapters import train_classifier_simple\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "\tmodel, train_loader, val_loader, optimizer, device,\n",
    "\tnum_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 99.52%\n",
      "Validation accuracy: 97.99%\n",
      "Test accuracy: 98.00%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存权重\n",
    "torch.save(model.state_dict(), \"LoRA_classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
